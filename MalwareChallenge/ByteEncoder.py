import sys
import os
import numpy
import scipy.optimize


class CAE(object):
    """
    A Contractive Auto-Encoder (CAE) with sigmoid input units and sigmoid hidden units.
    """
    def __init__(self, 
                 n_hiddens=1024,
                 jacobi_penalty=0.1,
                 learning_rate=0.1,
                 W=None,
                 b=None,
                 c=None,
                 batch_size=20,
                 epochs=20):
        self.n_hiddens = n_hiddens
        self.jacobi_penalty = jacobi_penalty
        self.learning_rate = learning_rate
        self.W = W
        self.b = b
        self.c = c
        self.batch_size = batch_size
        self.epochs = epochs

    def _sigmoid(self, x):
        return 1. / (1. + numpy.exp(-numpy.maximum(numpy.minimum(x, 30), -30)))

    def transform(self, x):
        return self._sigmoid(numpy.dot(x, self.W) + self.b)

    def decode(self, h):
        return self._sigmoid(numpy.dot(h, self.W.T) + self.c)

    def reconstruct(self, x):
        return self.decode(self.transform(x))

    def jacobian(self, x):
        h = self.transform(x)
        
        return (h * (1 - h))[:, :, None] * self.W.T

    def sample(self, x, sigma=1):
        h = self.transform(x)
        
        s = h * (1. - h)
        
        JJ = numpy.dot(self.W.T, self.W) * s[:, None, :] * s[:, :, None]
        
        alpha = numpy.random.normal(0, sigma, h.shape)
        
        delta = (alpha[:, :, None] * JJ).sum(1)
        
        return self.decode(h + delta)

    def loss(self, x, h=None, r=None):
        if h == None:
            h = self.transform(x)
        if r == None:
            r = self.decode(h)
        
        def _reconstruction_loss(h, r):
            return (- (x * numpy.log(r) + (1 - x) * numpy.log(1 - r)).sum(1)).sum()

        def _jacobi_loss(h):
            return (((h * (1 - h)) ** 2).sum(0) * self.W ** 2).sum()

        return (_reconstruction_loss(h, r) + self.jacobi_penalty * _jacobi_loss(h))

    def _fit(self, x):
        h = self.transform(x)
        r = self.decode(h)
        def _contraction_jacobian():
            a = (h * (1 - h)) ** 2 

            d = ((1 - 2 * h) * a * (self.W ** 2).sum(0)[None, :])

            b = numpy.dot(x.T / x.shape[0], d)

            c = a.mean(0) * self.W

            return (b + c), d.mean(0)
        
        def _reconstruction_jacobian():
            dr = (r - x) / x.shape[0]
            dd = numpy.dot(dr.T, h)
            dh = numpy.dot(dr, self.W) * h * (1. - h)
            de = numpy.dot(x.T, dh)

            return (dd + de), dr.sum(0), dh.sum(0)

        W_rec, c_rec, b_rec = _reconstruction_jacobian()
        W_con, b_con = _contraction_jacobian()
        
        self.W -= self.learning_rate * (W_rec + self.jacobi_penalty * W_con)
        self.b -= self.learning_rate * (b_rec + self.jacobi_penalty * b_con)
        self.c -= self.learning_rate * c_rec
        
        return self.loss(x, h, r)

    def fit(self, X, verbose=False, callback=None):
        if self.W == None:
            self.W = numpy.asarray(numpy.random.uniform(low=-4 * numpy.sqrt(6. / (X.shape[1] + self.n_hiddens)),
                high=4 * numpy.sqrt(6. / (X.shape[1] + self.n_hiddens)),
                size=(X.shape[1], self.n_hiddens)), dtype=X.dtype)
            self.b = numpy.zeros(self.n_hiddens, dtype=X.dtype)
            self.c = numpy.zeros(X.shape[1], dtype=X.dtype)
        
        inds = range(X.shape[0])
        
        numpy.random.shuffle(inds)
        
        n_batches = len(inds) / self.batch_size
        
        for epoch in range(self.epochs):
            loss = 0.
            for minibatch in range(n_batches):
                loss += self._fit(X[inds[minibatch::n_batches]])
            
            if verbose:
                print "Epoch %d, Loss = %.2f" % (epoch, loss / len(inds))
                sys.stdout.flush()
            
            if callback != None:
                callback(epoch)
